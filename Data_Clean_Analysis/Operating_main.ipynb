{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_data.csv'\n",
    "columns_to_keep = ['Run','Experiment','TaxID', 'spots', 'size_MB', 'ScientificName','Platform', 'Model', 'ReleaseDate', 'CenterName', 'Country','bases','LibrarySource','LibraryStrategy']\n",
    "data = pd.read_csv(file_path, usecols=columns_to_keep,dtype=str)\n",
    "columns_to_convert = ['size_MB', 'bases', 'spots',  'TaxID', 'ReleaseDate']\n",
    "data = data[data['size_MB'] != 'size_MB']\n",
    "columns_str = ['Run', 'Experiment', 'ScientificName', 'Platform', 'Model', 'CenterName', 'Country','LibrarySource','LibraryStrategy']\n",
    "data[columns_str] = data[columns_str].replace([None,'','nan'], pd.NA).fillna('Unknown')\n",
    "data[columns_str] = data[columns_str].astype(str)\n",
    "data[columns_to_convert] = data[columns_to_convert].replace([None,'','nan'], pd.NA).fillna(0)\n",
    "data[columns_to_convert] = data[columns_to_convert].astype(float)\n",
    "data = data.apply(lambda x: str(x)[:-1] if isinstance(x, str) and x.endswith(' ') else x)\n",
    "data = data.apply(lambda x: str(x)[:-1] if isinstance(x, str) and x.endswith(' ') else x)\n",
    "data = data.drop_duplicates(subset=['Run','Experiment'], keep='first')\n",
    "data[columns_to_convert] = data[columns_to_convert].astype('int64')\n",
    "filtered_data = data[data['Platform'] != 'Unknown']\n",
    "grouped_data = filtered_data.groupby('Model')['Platform'].unique()\n",
    "data = pd.merge(data, grouped_data, on='Model', how='inner')\n",
    "data = data.drop('Platform_x', axis=1)\n",
    "data = data.rename(columns={'Platform_y': 'Platform'})\n",
    "data['Platform'] = data['Platform'].astype(str).str.replace(\"[\", '')\n",
    "data['Platform'] = data['Platform'].astype(str).str.replace(\"]\", '')\n",
    "data['Platform'] = data['Platform'].astype(str).str.replace(\"'\", '')\n",
    "data['Platform'] = data['Platform'].astype(str).str.replace('\"', '')\n",
    "data = data.drop_duplicates(subset=['Run','Experiment'], keep='first')\n",
    "\n",
    "data.to_csv('SRA_graf_inf.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28124150, 6)\n",
      "Количество NaN значений в колонке \"Run\": 0\n",
      "Количество NaN значений в колонке \"Experiment\": 0\n",
      "Количество NaN значений в колонке \"TaxID\": 1469433\n",
      "Количество NaN значений в колонке \"Disease\": 28121593\n",
      "Количество NaN значений в колонке \"Sex\": 24299228\n",
      "Количество NaN значений в колонке \"Tumor\": 1468054\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_data.csv'\n",
    "columns_to_keep = ['Run','Experiment',\"TaxID\", \"Disease\",  \"Tumor\", \"Sex\"]\n",
    "data = pd.read_csv(file_path, usecols=columns_to_keep,dtype=str)\n",
    "data.replace(['None', '0', '','nan','OTHER','Other','Unknown',0], pd.NA, inplace=True)\n",
    "print(data.shape)\n",
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()\n",
    "    print(f'Количество NaN значений в колонке \"{column}\": {nan_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4866940, 6)\n",
      "Количество NaN значений в колонке \"Run\": 0\n",
      "Количество NaN значений в колонке \"Experiment\": 0\n",
      "Количество NaN значений в колонке \"TaxID\": 0\n",
      "Количество NaN значений в колонке \"Disease\": 4864390\n",
      "Количество NaN значений в колонке \"Sex\": 2720718\n",
      "Количество NaN значений в колонке \"Tumor\": 4725271\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_data.csv'\n",
    "columns_to_keep = ['Run','Experiment',\"TaxID\", \"Disease\",  \"Tumor\", \"Sex\"]\n",
    "data = pd.read_csv(file_path, usecols=columns_to_keep,dtype=str)\n",
    "data=data[data['TaxID']== '9606' ]\n",
    "data.replace(['None', '0', '','nan','OTHER','Other','Unknown',0,'no'], pd.NA, inplace=True)\n",
    "print(data.shape)\n",
    "for column in data.columns:\n",
    "    nan_count = data[column].isna().sum()\n",
    "    print(f'Количество NaN значений в колонке \"{column}\": {nan_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_data.csv'\n",
    "columns_to_keep = ['Run','Experiment','ScientificName','TaxID']\n",
    "data = pd.read_csv(file_path, usecols=columns_to_keep,dtype=str)\n",
    "filtered_data = data[~data['ScientificName'].str.contains('metagenom')]\n",
    "\n",
    "nan_count = filtered_data['TaxID'].isna().sum()\n",
    "\n",
    "print(\"Количество NaN значений в колонке 'taxid':\", nan_count)\n",
    "print(data.shape)\n",
    "\n",
    "empty_count = (data['LibraryStrategy'] == 'OTHER').sum()\n",
    "print(empty_count)\n",
    "total_rows = len(data)\n",
    "\n",
    "empty_percentage = (empty_count / total_rows) * 100\n",
    "print(empty_percentage)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма 'Count' для строк, где 'Count' > 10000: 25725510\n",
      "Сумма 'Count' для строк, где 'Count' > 1000: 27146665\n",
      "Сумма 'Count' для строк, где 'Count' > 100: 27852333\n",
      "Сумма 'Count' для строк, где 'Count' > 0: 28124143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_graf_inf.csv'\n",
    "data = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "def get_first_two_words(text):\n",
    "    if isinstance(text, str) and 'metagenome' in text.lower():\n",
    "        return \"metagenome\"\n",
    "    elif isinstance(text, str) :\n",
    "        words = text.split() \n",
    "        return ' '.join(words[:2]) if len(words) >= 2 else text\n",
    "    else: return \"\"\n",
    "def get_first_words(text):\n",
    "    if isinstance(text, str) and 'metagenome' in text.lower():\n",
    "        return \"metagenome\"\n",
    "    elif isinstance(text, str) :\n",
    "        words = text.split() \n",
    "        return ' '.join(words[:1]) if len(words) >= 1 else text\n",
    "    else: return \"\"\n",
    "data_words = data['ScientificName'].apply(get_first_two_words)\n",
    "\n",
    "counts = data_words.value_counts()\n",
    "\n",
    "counts_df = counts.reset_index()\n",
    "counts_df.columns = ['SC Combination', 'Count']\n",
    "\n",
    "\n",
    "counts_df.loc[counts_df['Count'] < 10000, 'SC Combination'] = counts_df.loc[counts_df['Count'] < 10000, 'SC Combination'].apply(get_first_words)\n",
    "counts_df = counts_df.groupby('SC Combination')['Count'].sum().reset_index()\n",
    "counts_df  = counts_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "sum_counts = counts_df.loc[counts_df['Count'] > 10000, 'Count'].sum()\n",
    "\n",
    "print(f\"Сумма 'Count' для строк, где 'Count' > 10000: {sum_counts}\") \n",
    "sum_counts = counts_df.loc[counts_df['Count'] > 1000, 'Count'].sum()\n",
    "\n",
    "print(f\"Сумма 'Count' для строк, где 'Count' > 1000: {sum_counts}\")\n",
    "\n",
    "sum_counts = counts_df.loc[counts_df['Count'] > 100, 'Count'].sum()\n",
    "\n",
    "print(f\"Сумма 'Count' для строк, где 'Count' > 100: {sum_counts}\")\n",
    "\n",
    "sum_counts = counts_df.loc[counts_df['Count'] > 0, 'Count'].sum()\n",
    "print(f\"Сумма 'Count' для строк, где 'Count' > 0: {sum_counts}\")\n",
    "\n",
    "counts_df.to_csv('SC_counts.csv', index=False, encoding='utf-8')\n",
    "\n",
    "data['SC'] = data['ScientificName'].apply(get_first_two_words)\n",
    "\n",
    "counts = data_words.value_counts()\n",
    "\n",
    "counts_df = counts.reset_index()\n",
    "counts_df.columns = ['SC', 'Count']\n",
    "data=pd.merge(data,counts_df,on='SC',how='inner')\n",
    "\n",
    "data.loc[data['Count'] < 10000, 'SC'] = data.loc[data['Count'] < 10000, 'SC'].apply(get_first_words)\n",
    "data = data.drop('Count', axis=1)\n",
    "data.to_csv('data_organism_class_tmp.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28124148, 6)\n",
      "Class_organism\n",
      "Vertebrates      9198075\n",
      "Viruses          6793311\n",
      "Metagenome       5691523\n",
      "Bacteria         2610942\n",
      "Plants           1728005\n",
      "Unknown           758286\n",
      "Invertebrates     651186\n",
      "Fungi             348098\n",
      "Protists          338341\n",
      "Archaea             6381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_keep = ['Run','Experiment','ScientificName','size_MB','SC']\n",
    "data = pd.read_csv('data_organism_class_tmp.csv', usecols=columns_to_keep,dtype=str)#tmp_csv\n",
    "\n",
    "data_class_org = pd.read_csv('class_org.csv',dtype=str)\n",
    "data['ScientificName_TMP'] = data['SC'].copy()\n",
    "data['SC'] = data['SC'].str.lower().str.replace(' ', '')\n",
    "data_class_org['SC'] = data_class_org['SC'].str.lower().str.replace(' ', '')\n",
    "\n",
    "data_main=pd.merge(data,data_class_org,how='outer',on='SC').drop_duplicates()\n",
    "data_main['Class_organism'] = data_main['Class_organism'].fillna('Unknown')\n",
    "\n",
    "data_main['SC'] = data_main['ScientificName_TMP'].copy()\n",
    "data_main = data_main.drop('ScientificName_TMP', axis=1)\n",
    "\n",
    "print(data_main.shape)\n",
    "data_main.to_csv('data_organism_class.csv', index=False, encoding='utf-8')\n",
    "print(data_main['Class_organism'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28124150, 7)\n",
      "Class_metagenome\n",
      "Human                    1807297\n",
      "Environment              1695369\n",
      "Animal                    806755\n",
      "metagenome                538040\n",
      "Plant                     464550\n",
      "Artificial ecosystems     379415\n",
      "Name: count, dtype: int64\n",
      "ScientificName\n",
      "soil metagenome                   777926\n",
      "human gut metagenome              694138\n",
      "metagenome                        529976\n",
      "gut metagenome                    364223\n",
      "human metagenome                  240470\n",
      "                                   ...  \n",
      "iron metagenome                        2\n",
      "moonmilk metagenome                    2\n",
      "desalination cell metagenome           1\n",
      "metal metagenome                       1\n",
      "chemical production metagenome         1\n",
      "Name: count, Length: 351, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data_organism_class.csv',dtype=str)\n",
    "data_metagenome_class = pd.read_csv('count_ScientificName_metagenome.csv',dtype=str)\n",
    "data_main=pd.merge(data,data_metagenome_class,how='outer',on='ScientificName').drop_duplicates()\n",
    "data_main = data_main.drop('Count', axis=1)\n",
    "data_main.to_csv('data_organism_class_plus_metagenome.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(data_main.shape)\n",
    "print(data_main['Class_metagenome'].value_counts())\n",
    "print(data_main[data_main['Class_organism'] == 'Metagenome']['ScientificName'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'SRA_graf_inf.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogFormatter\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "replacement_dict = {\n",
    "    'Western Sahara': 'W. Sahara',\n",
    "    'Democratic Republic of the Congo': 'Dem. Rep. Congo',\n",
    "    'French Southern and Antarctic Lands': 'Fr. S. Antarctic Lands',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'Eswatini': 'eSwatini',\n",
    "    'Equatorial Guinea': 'Eq. Guinea',\n",
    "    'South Korea': 'Korea, South',\n",
    "    'North Korea': 'Korea, North',\n",
    "    'Czech Republic': 'Czechia',\n",
    "    'Macedonia': 'North Macedonia',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',\n",
    "    'Northern Cyprus': 'N. Cyprus',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    \"Côte d'Ivoire\": 'Ivory Coast',\n",
    "    'United States': 'United States of America',\n",
    "    'United Kingdom': 'United Kingdom',\n",
    "    'French Southern and Antarctic Lands': 'Fr. S. Antarctic Lands',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'Korea, South': 'South Korea',\n",
    "    'Korea, North': 'North Korea',\n",
    "    'Czechia': 'Czech Republic',\n",
    "    'North Macedonia': 'Macedonia',\n",
    "    'Bosnia and Herz.': 'Bosnia and Herzegovina',\n",
    "    'N. Cyprus': 'Northern Cyprus',\n",
    "    'S. Sudan': 'South Sudan',\n",
    "}\n",
    "\n",
    "reversed_dict = {v: k for k, v in replacement_dict.items()}\n",
    "\n",
    "country_freq = data['Country'].value_counts().reset_index()\n",
    "country_freq.columns = ['name', 'Frequency']\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world['name'] = world['name'].replace(reversed_dict)\n",
    "\n",
    "unique_continents = world['name'].unique()\n",
    "world = pd.merge(world, country_freq, on='name', how='left')\n",
    "world_regions = world.drop_duplicates(subset='name', keep='first')\n",
    "world_regions = world_regions[['name', 'continent']]  # Use double square brackets or pass a list of column names\n",
    "world_regions.to_csv('world_regions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "with open('SRA_data.csv', mode='r', newline='', encoding='utf-8') as input_file:\n",
    "    with open('output.csv', mode='w', newline='', encoding='utf-8') as output_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        writer = csv.writer(output_file)\n",
    "        \n",
    "        headers = next(reader)\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row[23] == 'SYNTHETIC': \n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "file_path = 'SRA_data.csv'\n",
    "columns_to_keep = ['Run','Country']\n",
    "data = pd.read_csv(file_path, usecols=columns_to_keep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogFormatter\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "norm = LogNorm(vmin=1000, vmax=12000000)\n",
    "\n",
    "replacement_dict = {\n",
    "    'Western Sahara': 'W. Sahara',\n",
    "    'Democratic Republic of the Congo': 'Dem. Rep. Congo',\n",
    "    'French Southern and Antarctic Lands': 'Fr. S. Antarctic Lands',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'Eswatini': 'eSwatini',\n",
    "    'Equatorial Guinea': 'Eq. Guinea',\n",
    "    'South Korea': 'Korea, South',\n",
    "    'North Korea': 'Korea, North',\n",
    "    'Czech Republic': 'Czechia',\n",
    "    'Macedonia': 'North Macedonia',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',\n",
    "    'Northern Cyprus': 'N. Cyprus',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    \"Côte d'Ivoire\": 'Ivory Coast',\n",
    "    'United States': 'United States of America',\n",
    "    'United Kingdom': 'United Kingdom',\n",
    "    'French Southern and Antarctic Lands': 'Fr. S. Antarctic Lands',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'Korea, South': 'South Korea',\n",
    "    'Korea, North': 'North Korea',\n",
    "    'Czechia': 'Czech Republic',\n",
    "    'North Macedonia': 'Macedonia',\n",
    "    'Bosnia and Herz.': 'Bosnia and Herzegovina',\n",
    "    'N. Cyprus': 'Northern Cyprus',\n",
    "    'S. Sudan': 'South Sudan',\n",
    "}\n",
    "\n",
    "data['Country'] = data['Country'].replace(replacement_dict)\n",
    "\n",
    "country_freq = data['Country'].value_counts().reset_index()\n",
    "country_freq.columns = ['name', 'Frequency']\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "unique_continents = world['name'].unique()\n",
    "\n",
    "world = pd.merge(world, country_freq, on='name', how='left')\n",
    "\n",
    "world['Frequency'].fillna(1, inplace=True)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7), dpi=700)\n",
    "colors = [ (1, 1, 0.9), (1, 0.5, 0), (1, 0, 0)]  \n",
    "positions = [0, 0.33, 0.66, 1]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('custom_gradient', colors, N=256)\n",
    "\n",
    "plot = world.plot(column='Frequency', cmap=cmap, alpha=1 ,linewidth=0.8, ax=ax, edgecolor='0.8', legend=False, norm=norm)\n",
    "formatter = LogFormatter(10, labelOnlyBase=False)\n",
    "cbar = fig.colorbar(plot.get_children()[0], ax=ax, format=formatter, shrink=0.8)\n",
    "\n",
    "\n",
    "cbar.set_ticks([1000,10000, 100000, 1000000, 4000000, 12000000])\n",
    "cbar.set_ticklabels(['<1k','10k', '100k', '1M', '4M', '>12M'])\n",
    "\n",
    "ax.set_title('Heatmap by \"Run\" quantity')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "input_file_path = 'SRA_data.csv'\n",
    "output_file_path = 'small_SRA_data.csv'\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname,newline='', encoding='utf-8') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "n = 100000\n",
    "total_lines = file_len(input_file_path) - 1\n",
    "selected_lines = sorted(random.sample(range(1, total_lines + 1), n))\n",
    "\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "        open(output_file_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    writer.writerow(next(reader))\n",
    "\n",
    "    for line_number, row in enumerate(reader, start=1):\n",
    "        if line_number == selected_lines[0]:\n",
    "            writer.writerow(row)\n",
    "            selected_lines.pop(0)\n",
    "            if not selected_lines:\n",
    "                break\n",
    "\n",
    "print(\"Выборка завершена и сохранена в\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных значений в df1, которых нет в df2: 1467839\n",
      "Уникальных значений в df2, которых нет в df1: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path1 = 'SRA_data.csv'\n",
    "file_path2 = 'SRA_fulldata.csv'\n",
    "df1 = pd.read_csv(file_path1, usecols=['Experiment'])\n",
    "df2 = pd.read_csv(file_path2, usecols=['Experiment'])\n",
    "\n",
    "unique_values_df1 = df1['Experiment'].nunique()\n",
    "unique_values_df2 = df2['Experiment'].nunique()\n",
    "\n",
    "\n",
    "unique_values_only_in_df1 = df1[~df1['Experiment'].isin(df2['Experiment'])]\n",
    "unique_values_only_in_df2 = df2[~df2['Experiment'].isin(df1['Experiment'])]\n",
    "\n",
    "print(f'Уникальных значений в df1, которых нет в df2: {len(unique_values_only_in_df1)}')\n",
    "print(f'Уникальных значений в df2, которых нет в df1: {len(unique_values_only_in_df2)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26767311, 1)\n",
      "(25299478, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df1.drop_duplicates().shape)\n",
    "print(df2.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'small_SRA_data.csv'\n",
    "small_SRA_data = pd.read_csv(file_path,dtype=str)\n",
    "file_path = 'Run_Score.csv'\n",
    "Completeness_Data = pd.read_csv(file_path,dtype=str)\n",
    "merged_data = pd.merge(small_SRA_data, Completeness_Data, on='Run', how='inner')\n",
    "merged_data.to_csv('small_SRA_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import random\n",
    "csv.field_size_limit(100000000)\n",
    "input_file_path = 'SRA_data.csv'\n",
    "output_file_path = 'Completeness_Data.csv'\n",
    "def check_and_replace(value):\n",
    "    if value in ['nan', None, '', 'Unknown', 'Other', 'other', '0', '-1',0,-1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "with open(input_file_path, 'r', newline='', encoding='utf-8') as input_file:\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        writer = csv.writer(output_file)\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "        for row in reader:\n",
    "            first_column = row[0]          \n",
    "            processed_row = [first_column] + [check_and_replace(value) for value in row[1:]]\n",
    "            writer.writerow(processed_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28124150, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'Completeness_data.csv'\n",
    "data = pd.read_csv(file_path,dtype=str)\n",
    "data.drop(columns=['Tumor'], inplace=True)\n",
    "data.drop(columns=['Affection_Status'], inplace=True)\n",
    "data.drop(columns=['Analyte_Type'], inplace=True)\n",
    "data.drop(columns=['AssemblyName'], inplace=True)\n",
    "data.drop(columns=['Body_Site'], inplace=True)\n",
    "data.drop(columns=['Consent'], inplace=True)\n",
    "data.drop(columns=['InsertDev'], inplace=True)\n",
    "data.drop(columns=['InsertSize'], inplace=True)\n",
    "data.drop(columns=['Country'], inplace=True)\n",
    "data.drop(columns=['Study_Pubmed_id'], inplace=True)\n",
    "data.drop(columns=['Disease'], inplace=True)\n",
    "print(data.shape)\n",
    "def calculate_score(row):\n",
    "    score = int(sum([float(x) for x in row[1:]]) * 3.125)\n",
    "    return score\n",
    "\n",
    "data['Score'] = data.apply(calculate_score, axis=1)\n",
    "data[['Run', 'Score']].to_csv('Run_Score.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
